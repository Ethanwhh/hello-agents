# ============================================================================
# HelloAgents Chapter 11 - Agentic RL 环境变量配置文件
# ============================================================================
# 复制此文件为 .env 并填入你的API密钥和配置
# 系统要求：Python 3.10+ （必需）

# ============================================================================
# 🚀 统一配置格式（推荐）- 框架自动检测provider
# ============================================================================
# 只需配置以下4个通用环境变量，框架会自动识别LLM提供商：

# 模型名称
LLM_MODEL_ID=your-model-name

# API密钥
LLM_API_KEY=your-api-key-here

# 服务地址
LLM_BASE_URL=your-api-base-url

# 超时时间（可选，默认60秒）
LLM_TIMEOUT=60

# ============================================================================
# 🤖 强化学习训练配置
# ============================================================================

# --------------------------------
# 训练模型配置
# --------------------------------
# 基础模型（用于SFT和GRPO训练）
# 推荐使用小模型进行实验：Qwen3-0.6B, Qwen2.5-0.5B-Instruct
RL_BASE_MODEL=Qwen/Qwen3-0.6B

# 训练输出目录
RL_OUTPUT_DIR=./outputs

# --------------------------------
# 数据集配置
# --------------------------------
# 数据集名称（默认使用GSM8K数学推理数据集）
RL_DATASET_NAME=openai/gsm8k

# 数据集配置
RL_DATASET_CONFIG=main

# 数据集分割
RL_DATASET_SPLIT=train

# 最大样本数（用于快速实验，设为-1使用全部数据）
RL_MAX_SAMPLES=100

# --------------------------------
# SFT训练超参数
# --------------------------------
# 训练轮数
SFT_NUM_EPOCHS=3

# 批次大小
SFT_BATCH_SIZE=4

# 梯度累积步数
SFT_GRADIENT_ACCUMULATION_STEPS=4

# 学习率
SFT_LEARNING_RATE=2e-5

# 最大序列长度
SFT_MAX_SEQ_LENGTH=512

# LoRA配置
SFT_LORA_R=8
SFT_LORA_ALPHA=16
SFT_LORA_DROPOUT=0.05

# --------------------------------
# GRPO训练超参数
# --------------------------------
# 训练轮数
GRPO_NUM_EPOCHS=2

# 批次大小
GRPO_BATCH_SIZE=4

# 梯度累积步数
GRPO_GRADIENT_ACCUMULATION_STEPS=4

# 学习率
GRPO_LEARNING_RATE=1e-5

# 最大序列长度
GRPO_MAX_SEQ_LENGTH=512

# 每个prompt生成的响应数量
GRPO_NUM_GENERATIONS=4

# KL散度惩罚系数
GRPO_KL_COEF=0.1

# LoRA配置
GRPO_LORA_R=8
GRPO_LORA_ALPHA=16
GRPO_LORA_DROPOUT=0.05

# --------------------------------
# 奖励函数配置
# --------------------------------
# 奖励函数类型：accuracy, length_penalty, step_penalty
REWARD_TYPE=accuracy

# 长度惩罚系数（仅用于length_penalty）
REWARD_LENGTH_PENALTY=0.01

# 步骤惩罚系数（仅用于step_penalty）
REWARD_STEP_PENALTY=0.05

# --------------------------------
# 分布式训练配置
# --------------------------------
# 是否启用分布式训练
DISTRIBUTED_TRAINING=false

# 分布式训练策略：ddp, deepspeed_zero2, deepspeed_zero3
DISTRIBUTED_STRATEGY=ddp

# GPU数量（-1表示使用所有可用GPU）
NUM_GPUS=-1

# DeepSpeed配置文件路径（可选）
# DEEPSPEED_CONFIG=./accelerate_configs/deepspeed_zero2.yaml

# --------------------------------
# 监控与日志配置
# --------------------------------
# 是否启用TensorBoard
ENABLE_TENSORBOARD=true

# TensorBoard日志目录
TENSORBOARD_LOG_DIR=./logs/tensorboard

# 是否启用Wandb
ENABLE_WANDB=false

# Wandb项目名称
# WANDB_PROJECT=helloagents-rl

# Wandb API密钥（获取方式：https://wandb.ai/authorize）
# WANDB_API_KEY=your_wandb_api_key_here

# 日志级别：DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# 日志保存间隔（步数）
LOGGING_STEPS=10

# 模型保存间隔（步数）
SAVE_STEPS=100

# 评估间隔（步数）
EVAL_STEPS=100

# --------------------------------
# 硬件与性能配置
# --------------------------------
# 混合精度训练：no, fp16, bf16
MIXED_PRECISION=bf16

# 梯度检查点（节省显存）
GRADIENT_CHECKPOINTING=true

# DataLoader工作进程数
DATALOADER_NUM_WORKERS=4

# 是否固定随机种子
SEED=42

# ================================
# HuggingFace API 配置
# ================================
# HuggingFace Token - 用于下载模型和数据集
# 获取方式：https://huggingface.co/settings/tokens
HF_TOKEN=

# HuggingFace镜像站点（可选，用于加速下载）
# 中国大陆用户可以使用以下镜像：
# HF_ENDPOINT=https://hf-mirror.com

# ================================
# 模型缓存配置
# ================================
# HuggingFace模型缓存目录
# HF_HOME=~/.cache/huggingface

# Transformers缓存目录
# TRANSFORMERS_CACHE=~/.cache/huggingface/transformers

# ============================================================================
# 📝 配置说明
# ============================================================================
# 
# 1. 基础配置：
#    - 必须配置LLM相关变量（LLM_MODEL_ID, LLM_API_KEY, LLM_BASE_URL）
#    - RL_BASE_MODEL建议使用小模型（如Qwen3-0.6B）进行实验
# 
# 2. 训练配置：
#    - 快速实验：设置RL_MAX_SAMPLES=100, SFT_NUM_EPOCHS=1, GRPO_NUM_EPOCHS=1
#    - 完整训练：设置RL_MAX_SAMPLES=-1, 增加训练轮数
# 
# 3. 分布式训练：
#    - 单GPU：DISTRIBUTED_TRAINING=false
#    - 多GPU：DISTRIBUTED_TRAINING=true, 选择合适的DISTRIBUTED_STRATEGY
# 
# 4. 监控：
#    - 本地监控：ENABLE_TENSORBOARD=true
#    - 云端监控：ENABLE_WANDB=true, 配置WANDB_API_KEY
# 
# 5. 性能优化：
#    - 显存不足：启用GRADIENT_CHECKPOINTING, 减小BATCH_SIZE
#    - 加速训练：使用MIXED_PRECISION=bf16, 增加GRADIENT_ACCUMULATION_STEPS
# 
# ============================================================================

